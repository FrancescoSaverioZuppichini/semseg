{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Read data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build computation graph - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare training sample\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_truth = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "# Model componnents\n",
    "def weight_variable(shape):\n",
    "    initial_value = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial_value)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial_value = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial_value)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# 1st convolution layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "#W_conv1_shaped = tf.reshape(W_conv1, [-1, 5, 5, 1])\n",
    "#print W_conv1_shaped.get_shape()\n",
    "#tf.image_summary('filter1', W_conv1_shaped, 32)\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_conv1_CH1 = h_conv1[:,:,:,:1]\n",
    "#print h_conv1_CH1.get_shape()\n",
    "tf.image_summary('convol_layer1_CH1', h_conv1_CH1, 3)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "#h_pool1_CH1 = h_pool1[:,:,:,:1]\n",
    "#tf.image_summary('pool_layer1_CH1', h_pool1_CH1, 10)\n",
    "\n",
    "# 2nd convolution layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "#W_conv2_shaped = tf.reshape(W_conv2, [-1, 5, 5, 1])\n",
    "#print W_conv2_shaped.get_shape()\n",
    "#tf.image_summary('filter2', W_conv2_shaped, 32)\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_conv2_CH1 = h_conv2[:,:,:,:1]\n",
    "tf.image_summary('convol_layer2_CH1', h_conv2_CH1, 3)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# Fully-connected layer\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# Readout\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_predict = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using L2-norm regularization.\n",
      "Traning ...\n",
      "step 100, training accuracy 0.839999973774\n",
      "step 200, training accuracy 0.879999995232\n",
      "step 300, training accuracy 0.860000014305\n",
      "step 400, training accuracy 0.980000019073\n",
      "step 500, training accuracy 0.939999997616\n",
      "Training done!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "mini_batch = 50\n",
    "iterations = 501\n",
    "WITH_2NORM = 1\n",
    "KEEP_PRO = 0.5\n",
    "\n",
    "# Training method\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_predict, y_truth))\n",
    "\n",
    "# Declare keep_pro_ constant\n",
    "KEEP_PRO_ = 0.0\n",
    "\n",
    "if not WITH_2NORM:\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "    tf.scalar_summary('loss', cross_entropy)\n",
    "    KEEP_PRO_ = KEEP_PRO\n",
    "else:\n",
    "    regularization = tf.nn.l2_loss(W_fc2) + tf.nn.l2_loss(b_fc2)\n",
    "    loss = cross_entropy + regularization\n",
    "    tf.scalar_summary('loss', loss)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    KEEP_PRO_ = 1.0\n",
    "\n",
    "# Prediction and Testing method\n",
    "prediction_result = tf.equal(tf.argmax(y_predict,1), tf.argmax(y_truth,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction_result, tf.float32))\n",
    "tf.scalar_summary('accuracy', accuracy)\n",
    "\n",
    "# Add summary writer\n",
    "merged = tf.merge_all_summaries()\n",
    "#train_writer = tf.train.SummaryWriter('MNIST_data/', sess.graph)\n",
    "\n",
    "# Initialize variables\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "train_writer = tf.train.SummaryWriter('MNIST_data/', sess.graph)\n",
    "if not WITH_2NORM:\n",
    "    print \"Using Dropout regularization, dropout %f\".format(KEEP_PRO_)\n",
    "else:\n",
    "    print \"Using L2-norm regularization.\"\n",
    "\n",
    "# Run training\n",
    "print \"Traning ...\"\n",
    "for i in range(iterations):\n",
    "    batch_x, batch_y = mnist.train.next_batch(mini_batch)\n",
    "    if i >= 100 and i%100 == 0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={x: batch_x, y_truth: batch_y, keep_prob: 1.0})\n",
    "        print \"step {0}, training accuracy {1}\".format(i,train_accuracy)\n",
    "    if i >= 10 and i%10 == 0:\n",
    "        summary, _ = sess.run([merged, train_step], feed_dict={x: batch_x, y_truth: batch_y, keep_prob: KEEP_PRO_})\n",
    "        train_writer.add_summary(summary, i)\n",
    "    else:\n",
    "        sess.run(train_step, feed_dict={x: batch_x, y_truth: batch_y, keep_prob: KEEP_PRO_})\n",
    "train_writer.flush()\n",
    "train_writer.close()\n",
    "print \"Training done!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ...\n",
      "Test Accuracy:  0.9446\n"
     ]
    }
   ],
   "source": [
    "# Run testing\n",
    "print \"Testing ...\"\n",
    "print 'Test Accuracy: ', sess.run(accuracy, feed_dict={x: mnist.test.images, y_truth: mnist.test.labels, keep_prob: 1.0})\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
